<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Dgx-Spark | Brandon's Technical Blog</title>
<meta name=keywords content><meta name=description content="Deep dives into performance engineering, systems debugging, and AI infrastructure"><meta name=author content="Brandon Geraci"><link rel=canonical href=https://brandonrc.github.io/blog/series/dgx-spark/><link crossorigin=anonymous href=/blog/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://brandonrc.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://brandonrc.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://brandonrc.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://brandonrc.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://brandonrc.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://brandonrc.github.io/blog/series/dgx-spark/index.xml title=rss><link rel=alternate hreflang=en href=https://brandonrc.github.io/blog/series/dgx-spark/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://brandonrc.github.io/blog/series/dgx-spark/"><meta property="og:site_name" content="Brandon's Technical Blog"><meta property="og:title" content="Dgx-Spark"><meta property="og:description" content="Deep dives into performance engineering, systems debugging, and AI infrastructure"><meta property="og:locale" content="en-us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dgx-Spark"><meta name=twitter:description content="Deep dives into performance engineering, systems debugging, and AI infrastructure"></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://brandonrc.github.io/blog/ accesskey=h title="Brandon's Technical Blog (Alt + H)">Brandon's Technical Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://brandonrc.github.io/blog/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://brandonrc.github.io/blog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://brandonrc.github.io/blog/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://brandonrc.github.io/blog/>Home</a>&nbsp;»&nbsp;<a href=https://brandonrc.github.io/blog/series/>Series</a></div><h1>Dgx-Spark
<a href=/blog/series/dgx-spark/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>DGX Spark Deep Dive</h2></header><div class=entry-content><p>A 6-part series investigating why Docker containers use 20-30GB more memory than native execution on Grace Blackwell’s unified memory architecture.
Overview When YouTube reviewers complained about DGX Spark performance, they blamed the hardware. I dug deeper and found the real culprit: Docker’s cgroups double-counting unified memory.
Key Findings Container overhead: 20-30 GB more memory usage in Docker KV cache reduction: 40-63% less KV cache in containers (1.7-2.7x reduction) Performance: Identical throughput - no speed penalty Root cause: Docker’s cgroups double-count unified memory on Grace Blackwell Solution: Use native execution for large models on unified memory architectures The Series The Mystery - YouTubers blamed NVIDIA hardware without technical analysis MPI and Chroot Nightmare - Setting up proper test environments The Unified Memory Revelation - Why Docker’s cgroups double-count unified memory The Data: 60 Runs Don’t Lie - Comprehensive benchmark results across 3 models What I Learned (And What’s Next) - Conclusions and Phase 2 preview KV Cache Deep Dive - The 2x reduction mystery explained Related Resources Benchmark Code & Data: benchmark-spark Interactive Results: brandonrc.github.io/benchmark-spark Impact This investigation demonstrates the importance of understanding the full stack - from hardware architecture to kernel subsystems to container runtimes. What looked like a hardware problem was actually a software architecture mismatch.
...</p></div><footer class=entry-footer><span title='2025-11-08 10:00:00 +0000 +0000'>November 8, 2025</span>&nbsp;·&nbsp;<span>1 min</span>&nbsp;·&nbsp;<span>207 words</span>&nbsp;·&nbsp;<span>Brandon Geraci</span></footer><a class=entry-link aria-label="post link to DGX Spark Deep Dive" href=https://brandonrc.github.io/blog/projects/dgx-spark/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>KV Cache Deep Dive: The 2x Reduction Mystery</h2></header><div class=entry-content><p>The Pattern That Demands Explanation In Part 4, I showed you 60 benchmark runs that revealed a consistent pattern. But one table in particular kept me up at night:
Model Native KV Container KV Reduction Factor DeepSeek-7B 44.31 GiB 16.57 GiB 2.7x less GPT-OSS-120B 43.19 GiB 23.65 GiB 1.8x less Qwen2.5-72B 44.71 GiB 26.72 GiB 1.7x less Two questions immediately jump out:
Why do containers consistently allocate ~2x less KV cache? (The main mystery) Why do all native runs converge to ~44 GiB? (The secondary puzzle) Let’s answer both.
...</p></div><footer class=entry-footer><span title='2025-11-08 15:00:00 +0000 +0000'>November 8, 2025</span>&nbsp;·&nbsp;<span>18 min</span>&nbsp;·&nbsp;<span>3687 words</span>&nbsp;·&nbsp;<span>Brandon Geraci</span></footer><a class=entry-link aria-label="post link to KV Cache Deep Dive: The 2x Reduction Mystery" href=https://brandonrc.github.io/blog/projects/dgx-spark/06-kv-cache-deep-dive/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>What I Learned (And What's Next)</h2></header><div class=entry-content><p>The Journey So Far Let’s recap this wild ride:
The Problem: YouTube reviewers blamed NVIDIA hardware for being slow The Investigation: I found 20-30GB memory overhead in Docker containers The Environment Setup: MPI and chroot configuration nightmare The Revelation: Docker’s cgroups double-count unified memory The Data: 60 runs confirmed the pattern consistently Now, what do I actually do with this knowledge?
Key Finding: Don’t Blame the Hardware The most important lesson from this entire investigation:
...</p></div><footer class=entry-footer><span title='2025-11-08 14:00:00 +0000 +0000'>November 8, 2025</span>&nbsp;·&nbsp;<span>5 min</span>&nbsp;·&nbsp;<span>964 words</span>&nbsp;·&nbsp;<span>Brandon Geraci</span></footer><a class=entry-link aria-label="post link to What I Learned (And What's Next)" href=https://brandonrc.github.io/blog/projects/dgx-spark/05-what-we-learned/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>The Data: 60 Runs Don't Lie</h2></header><div class=entry-content><p>The Comprehensive Test Plan Anecdotes are interesting. Single data points are suggestive. But 60 benchmark runs across multiple models? That’s science.
Here’s what I did:
Test Matrix 3 Models: DeepSeek-R1-Distill-Qwen-7B (7 billion parameters) Qwen2.5-72B-Instruct (72 billion parameters) GPT-OSS-120B (120 billion parameters, MXFP4 quantized) 2 Environments: Native (chroot) vs Container (Docker) 10 Iterations per model per environment Total: 60 benchmark runs Methodology Framework: TensorRT-LLM (trtllm-bench CLI) Workload: 50 requests, 128 output tokens per request Cooldown: 5 minutes + GPU temp check (&lt; 45°C) between runs Duration: ~14 hours total Metrics: Peak memory, KV cache, throughput, latency, temperature DeepSeek-7B Results My smallest model, but the most shocking results:
...</p></div><footer class=entry-footer><span title='2025-11-08 13:00:00 +0000 +0000'>November 8, 2025</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>818 words</span>&nbsp;·&nbsp;<span>Brandon Geraci</span></footer><a class=entry-link aria-label="post link to The Data: 60 Runs Don't Lie" href=https://brandonrc.github.io/blog/projects/dgx-spark/04-the-data/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>The Unified Memory Revelation: Why Docker Double-Counts</h2></header><div class=entry-content><p>The Question That Started It All After getting both Docker and native environments working, I could finally run proper benchmarks. But I kept asking myself:
“Where is the 26GB going?”
It wasn’t CPU overhead - containers don’t add 26GB of process memory.
It wasn’t the Docker daemon - that’s tiny.
It wasn’t duplicate libraries - bind mounts prevent that.
So… where?
Traditional GPU Systems (The Old Way) Let’s start with how most GPU systems work. Take an NVIDIA H100 or A100:
...</p></div><footer class=entry-footer><span title='2025-11-08 12:00:00 +0000 +0000'>November 8, 2025</span>&nbsp;·&nbsp;<span>8 min</span>&nbsp;·&nbsp;<span>1685 words</span>&nbsp;·&nbsp;<span>Brandon Geraci</span></footer><a class=entry-link aria-label="post link to The Unified Memory Revelation: Why Docker Double-Counts" href=https://brandonrc.github.io/blog/projects/dgx-spark/03-unified-memory-revelation/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Down the Rabbit Hole: The MPI and Chroot Nightmare</h2></header><div class=entry-content><p>The Simple Plan (That Wasn’t Simple) After seeing the mysterious 26GB memory overhead in Docker, my plan was straightforward:
Extract the container’s filesystem Run the same Python scripts natively Compare the results Done! Ha. Hahahaha. No.
Attempt 1: Just Run It My first thought: “Let’s just run the Docker scripts on my system. How hard can it be?”
1 2 python /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py \ --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B What I got: Runtime nightmare.
...</p></div><footer class=entry-footer><span title='2025-11-08 11:00:00 +0000 +0000'>November 8, 2025</span>&nbsp;·&nbsp;<span>6 min</span>&nbsp;·&nbsp;<span>1141 words</span>&nbsp;·&nbsp;<span>Brandon Geraci</span></footer><a class=entry-link aria-label="post link to Down the Rabbit Hole: The MPI and Chroot Nightmare" href=https://brandonrc.github.io/blog/projects/dgx-spark/02-mpi-chroot-nightmare/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>The Mystery: Don't Just Blame the Hardware</h2></header><div class=entry-content><p>The YouTube Problem If you search for “DGX Spark performance” on YouTube, you’ll find plenty of videos with clickbait titles like “NVIDIA’s $X Machine is a DISAPPOINTMENT” or “Grace Blackwell: Overhyped and Underdelivering.”
And that really bothers me.
Not because I’m an NVIDIA fanboy (I’m not), but because none of these reviewers provided a technical explanation of why performance wasn’t meeting expectations. They just pointed at benchmark numbers, said “slow,” and moved on. No investigation into kernel settings, driver versions, container configurations, or software stack optimization. Just… blame the hardware.
...</p></div><footer class=entry-footer><span title='2025-11-08 10:00:00 +0000 +0000'>November 8, 2025</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>688 words</span>&nbsp;·&nbsp;<span>Brandon Geraci</span></footer><a class=entry-link aria-label="post link to The Mystery: Don't Just Blame the Hardware" href=https://brandonrc.github.io/blog/projects/dgx-spark/01-the-mystery/></a></article></main><footer class=footer><span>&copy; 2025 <a href=https://brandonrc.github.io/blog/>Brandon's Technical Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script></body></html>