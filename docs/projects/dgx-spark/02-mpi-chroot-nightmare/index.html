<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Down the Rabbit Hole: The MPI and Chroot Nightmare | Brandon's Technical Blog</title><meta name=keywords content="dgx-spark,mpi,chroot,debugging"><meta name=description content="Sometimes the hardest part of debugging isn't finding the bug - it's just getting to a clean test environment. Here's how we spent hours fighting MPI libraries and chroot configurations before we could even start benchmarking."><meta name=author content="Brandon Geraci"><link rel=canonical href=https://brandongeraci.com/projects/dgx-spark/02-mpi-chroot-nightmare/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://brandongeraci.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://brandongeraci.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://brandongeraci.com/favicon-32x32.png><link rel=apple-touch-icon href=https://brandongeraci.com/apple-touch-icon.png><link rel=mask-icon href=https://brandongeraci.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://brandongeraci.com/projects/dgx-spark/02-mpi-chroot-nightmare/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="https://brandongeraci.com/projects/dgx-spark/02-mpi-chroot-nightmare/"><meta property="og:site_name" content="Brandon's Technical Blog"><meta property="og:title" content="Down the Rabbit Hole: The MPI and Chroot Nightmare"><meta property="og:description" content="Sometimes the hardest part of debugging isn't finding the bug - it's just getting to a clean test environment. Here's how we spent hours fighting MPI libraries and chroot configurations before we could even start benchmarking."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2025-11-08T11:00:00+00:00"><meta property="article:modified_time" content="2025-11-08T11:00:00+00:00"><meta property="article:tag" content="Dgx-Spark"><meta property="article:tag" content="Mpi"><meta property="article:tag" content="Chroot"><meta property="article:tag" content="Debugging"><meta property="og:see_also" content="https://brandongeraci.com/projects/dgx-spark/"><meta property="og:see_also" content="https://brandongeraci.com/projects/dgx-spark/06-kv-cache-deep-dive/"><meta property="og:see_also" content="https://brandongeraci.com/projects/dgx-spark/05-what-we-learned/"><meta property="og:see_also" content="https://brandongeraci.com/projects/dgx-spark/04-the-data/"><meta property="og:see_also" content="https://brandongeraci.com/projects/dgx-spark/03-unified-memory-revelation/"><meta property="og:see_also" content="https://brandongeraci.com/projects/dgx-spark/01-the-mystery/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Down the Rabbit Hole: The MPI and Chroot Nightmare"><meta name=twitter:description content="Sometimes the hardest part of debugging isn't finding the bug - it's just getting to a clean test environment. Here's how we spent hours fighting MPI libraries and chroot configurations before we could even start benchmarking."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://brandongeraci.com/projects/"},{"@type":"ListItem","position":2,"name":"DGX Spark Deep Dive","item":"https://brandongeraci.com/projects/dgx-spark/"},{"@type":"ListItem","position":3,"name":"Down the Rabbit Hole: The MPI and Chroot Nightmare","item":"https://brandongeraci.com/projects/dgx-spark/02-mpi-chroot-nightmare/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Down the Rabbit Hole: The MPI and Chroot Nightmare","name":"Down the Rabbit Hole: The MPI and Chroot Nightmare","description":"Sometimes the hardest part of debugging isn't finding the bug - it's just getting to a clean test environment. Here's how we spent hours fighting MPI libraries and chroot configurations before we could even start benchmarking.","keywords":["dgx-spark","mpi","chroot","debugging"],"articleBody":"The Simple Plan (That Wasn’t Simple) After seeing the mysterious 26GB memory overhead in Docker, my plan was straightforward:\nExtract the container’s filesystem Run the same Python scripts natively Compare the results Done! Ha. Hahahaha. No.\nAttempt 1: Just Run It My first thought: “Let’s just run the Docker scripts on my system. How hard can it be?”\n1 2 python /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py \\ --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B What I got: Runtime nightmare.\nModuleNotFoundError: No module named 'tensorrt_llm' ImportError: libmpi.so.40: cannot open shared object file CUDA Error: no CUDA-capable device is detected Of course. The container has TensorRT-LLM, specific CUDA versions, MPI libraries, and about a million other dependencies that my host system doesn’t have (or has different versions of).\nOkay, fine. Let’s extract the container and use chroot.\nExtracting the Container Docker containers are just fancy tarballs with layers. To extract:\n1 2 3 4 5 6 7 8 # Export the container filesystem docker create --name temp nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev docker export temp \u003e container.tar docker rm temp # Extract to a directory mkdir -p /home/khan/container-rootfs sudo tar -xf container.tar -C /home/khan/container-rootfs Now I have the entire container filesystem at /home/khan/container-rootfs/. Cool!\nThe MPI Library Hunt Begins Time to run something:\n1 sudo chroot /home/khan/container-rootfs python3 /workspace/benchmarks/trtllm_benchmark.py New error:\n[TRT-LLM] [I] MPI size: 1, rank: 0 MPI Error: undefined symbol: ucs_config_doc_nop What? Let me check which MPI it’s using:\n1 2 which mpirun # /usr/bin/mpirun ← System MPI! Ah. The chroot is finding the system’s MPI (installed at /usr/bin/mpirun) instead of the container’s MPI (at /opt/hpcx/ompi/bin/mpirun inside the rootfs).\nThe PATH Dance TensorRT-LLM uses HPC-X OpenMPI from NVIDIA. The container has it at /opt/hpcx/ompi/. But when I chroot, the PATH still points to system binaries first.\nSolution: Explicitly set PATH to prioritize container binaries.\n1 export PATH=\"/opt/hpcx/ompi/bin:$PATH\" Run again… new error:\nmpirun: error while loading shared libraries: libucs.so.0: cannot open shared object file The MPI binary is now correct, but it can’t find its libraries!\nThe LD_LIBRARY_PATH Saga The container’s MPI needs libraries from /opt/hpcx/ompi/lib/, but LD_LIBRARY_PATH doesn’t include it.\n1 export LD_LIBRARY_PATH=\"/opt/hpcx/ompi/lib:$LD_LIBRARY_PATH\" Run again… DIFFERENT error:\nsymbol lookup error: /opt/hpcx/ompi/lib/libucc.so.1: undefined symbol: ucp_worker_progress Wait, what? Now it’s finding the HPC-X libraries but they’re conflicting with system libraries!\nThe Real Problem Here’s what was happening:\nThe system has OpenMPI installed (libmpi.so) The container has HPC-X OpenMPI (/opt/hpcx/ompi/lib/libmpi.so) TensorRT-LLM needs the HPC-X version But the dynamic linker was mixing system and container libraries The fix: Put container libraries at the FRONT of LD_LIBRARY_PATH:\n1 export LD_LIBRARY_PATH=\"/opt/hpcx/ompi/lib:/usr/local/lib/python3.12/dist-packages/tensorrt_libs:$LD_LIBRARY_PATH\" Finally, MPI works!\nThe CUDA Version Surprise Got MPI working. Started a benchmark. New error:\nRuntimeError: Triton only supports CUDA 10.0 or higher, but got CUDA version: 13.0 Wait, CUDA 13.0? We’re using CUDA 12.9!\nTurns out: The system symlink /usr/local/cuda pointed to CUDA 13.0. The container needs CUDA 12.9.\nFix:\n1 2 export CUDA_HOME=\"/usr/local/cuda-12.9\" export PATH=\"/usr/local/cuda-12.9/bin:$PATH\" The Full Chroot Wrapper Script After all this pain, I created a proper chroot wrapper that handles everything:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash SCRIPT_DIR=\"/home/khan/container-rootfs\" # Mount necessary filesystems sudo mount -t proc /proc \"${SCRIPT_DIR}/proc\" sudo mount --rbind /sys \"${SCRIPT_DIR}/sys\" sudo mount --rbind /dev \"${SCRIPT_DIR}/dev\" sudo mount --bind /home \"${SCRIPT_DIR}/home\" sudo mount --bind /data \"${SCRIPT_DIR}/data\" # DNS resolution sudo cp /etc/resolv.conf \"${SCRIPT_DIR}/etc/resolv.conf\" # Run in chroot with proper environment sudo chroot \"${SCRIPT_DIR}\" /usr/bin/env -i \\ HOME=/root \\ PATH=\"/opt/hpcx/ompi/bin:/usr/local/cuda-12.9/bin:/usr/local/bin:/usr/bin:/bin\" \\ LD_LIBRARY_PATH=\"/opt/hpcx/ompi/lib:/usr/local/lib/python3.12/dist-packages/tensorrt_libs:/usr/local/cuda-12.9/lib64\" \\ CUDA_HOME=\"/usr/local/cuda-12.9\" \\ PYTHONPATH=\"/usr/local/lib/python3.12/dist-packages\" \\ \"$@\" # Cleanup: unmount everything sudo umount \"${SCRIPT_DIR}/data\" sudo umount \"${SCRIPT_DIR}/home\" sudo umount \"${SCRIPT_DIR}/dev\" sudo umount \"${SCRIPT_DIR}/sys\" sudo umount \"${SCRIPT_DIR}/proc\" Now I can run:\n1 ./run_in_rootfs.sh python3 /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py And it actually works.\nLessons Learned Library paths matter: System vs container libraries will bite you Environment is everything: PATH, LD_LIBRARY_PATH, CUDA_HOME all critical MPI is picky: HPC-X OpenMPI isn’t interchangeable with system OpenMPI Filesystem mounts: Need /proc, /sys, /dev, /home, and /data bind mounts DNS matters: Even forgot /etc/resolv.conf initially! The Hard Part Is Over… Right? With a working chroot environment, I could finally start benchmarking. But getting here took hours of debugging library paths and runtime errors.\nSometimes I wonder if this is why people just accept Docker’s overhead - at least it works out of the box!\nBut now that we have both Docker and native environments working, we can actually compare them fairly. And that’s where things get interesting…\nIn the next post: What Grace Blackwell’s unified memory architecture actually means, and why Docker’s cgroups don’t understand it.\nPrevious: ← Part 1: The Mystery\nNext: Part 3: The Unified Memory Revelation →\nGitHub Repo: benchmark-spark\n","wordCount":"764","inLanguage":"en","datePublished":"2025-11-08T11:00:00Z","dateModified":"2025-11-08T11:00:00Z","author":{"@type":"Person","name":"Brandon Geraci"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://brandongeraci.com/projects/dgx-spark/02-mpi-chroot-nightmare/"},"publisher":{"@type":"Organization","name":"Brandon's Technical Blog","logo":{"@type":"ImageObject","url":"https://brandongeraci.com/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://brandongeraci.com/ accesskey=h title="Brandon's Technical Blog (Alt + H)">Brandon's Technical Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://brandongeraci.com/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://brandongeraci.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://brandongeraci.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://brandongeraci.com/>Home</a>&nbsp;»&nbsp;<a href=https://brandongeraci.com/projects/>Projects</a>&nbsp;»&nbsp;<a href=https://brandongeraci.com/projects/dgx-spark/>DGX Spark Deep Dive</a></div><h1 class="post-title entry-hint-parent">Down the Rabbit Hole: The MPI and Chroot Nightmare</h1><div class=post-description>Sometimes the hardest part of debugging isn't finding the bug - it's just getting to a clean test environment. Here's how we spent hours fighting MPI libraries and chroot configurations before we could even start benchmarking.</div><div class=post-meta><span title='2025-11-08 11:00:00 +0000 +0000'>November 8, 2025</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>764 words</span>&nbsp;·&nbsp;<span>Brandon Geraci</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#the-simple-plan-that-wasnt-simple>The Simple Plan (That Wasn&rsquo;t Simple)</a></li><li><a href=#attempt-1-just-run-it>Attempt 1: Just Run It</a></li><li><a href=#extracting-the-container>Extracting the Container</a></li><li><a href=#the-mpi-library-hunt-begins>The MPI Library Hunt Begins</a></li><li><a href=#the-path-dance>The PATH Dance</a></li><li><a href=#the-ld_library_path-saga>The LD_LIBRARY_PATH Saga</a></li><li><a href=#the-real-problem>The Real Problem</a></li><li><a href=#the-cuda-version-surprise>The CUDA Version Surprise</a></li><li><a href=#the-full-chroot-wrapper-script>The Full Chroot Wrapper Script</a></li><li><a href=#lessons-learned>Lessons Learned</a></li><li><a href=#the-hard-part-is-over-right>The Hard Part Is Over&mldr; Right?</a></li></ul></nav></div></details></div><div class=post-content><h2 id=the-simple-plan-that-wasnt-simple>The Simple Plan (That Wasn&rsquo;t Simple)<a hidden class=anchor aria-hidden=true href=#the-simple-plan-that-wasnt-simple>#</a></h2><p>After seeing the mysterious 26GB memory overhead in Docker, my plan was straightforward:</p><ol><li>Extract the container&rsquo;s filesystem</li><li>Run the same Python scripts natively</li><li>Compare the results</li><li>Done!</li></ol><p>Ha. Hahahaha. <strong>No.</strong></p><h2 id=attempt-1-just-run-it>Attempt 1: Just Run It<a hidden class=anchor aria-hidden=true href=#attempt-1-just-run-it>#</a></h2><p>My first thought: &ldquo;Let&rsquo;s just run the Docker scripts on my system. How hard can it be?&rdquo;</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
</span></span></code></pre></td></tr></table></div></div><p>What I got: <strong>Runtime nightmare</strong>.</p><pre tabindex=0><code>ModuleNotFoundError: No module named &#39;tensorrt_llm&#39;
ImportError: libmpi.so.40: cannot open shared object file
CUDA Error: no CUDA-capable device is detected
</code></pre><p>Of course. The container has TensorRT-LLM, specific CUDA versions, MPI libraries, and about a million other dependencies that my host system doesn&rsquo;t have (or has different versions of).</p><p>Okay, fine. Let&rsquo;s extract the container and use chroot.</p><h2 id=extracting-the-container>Extracting the Container<a hidden class=anchor aria-hidden=true href=#extracting-the-container>#</a></h2><p>Docker containers are just fancy tarballs with layers. To extract:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Export the container filesystem</span>
</span></span><span class=line><span class=cl>docker create --name temp nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev
</span></span><span class=line><span class=cl>docker <span class=nb>export</span> temp &gt; container.tar
</span></span><span class=line><span class=cl>docker rm temp
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Extract to a directory</span>
</span></span><span class=line><span class=cl>mkdir -p /home/khan/container-rootfs
</span></span><span class=line><span class=cl>sudo tar -xf container.tar -C /home/khan/container-rootfs
</span></span></code></pre></td></tr></table></div></div><p>Now I have the entire container filesystem at <code>/home/khan/container-rootfs/</code>. Cool!</p><h2 id=the-mpi-library-hunt-begins>The MPI Library Hunt Begins<a hidden class=anchor aria-hidden=true href=#the-mpi-library-hunt-begins>#</a></h2><p>Time to run something:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>sudo chroot /home/khan/container-rootfs python3 /workspace/benchmarks/trtllm_benchmark.py
</span></span></code></pre></td></tr></table></div></div><p>New error:</p><pre tabindex=0><code>[TRT-LLM] [I] MPI size: 1, rank: 0
MPI Error: undefined symbol: ucs_config_doc_nop
</code></pre><p>What? Let me check which MPI it&rsquo;s using:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>which mpirun
</span></span><span class=line><span class=cl><span class=c1># /usr/bin/mpirun  ← System MPI!</span>
</span></span></code></pre></td></tr></table></div></div><p>Ah. The chroot is finding the <strong>system&rsquo;s MPI</strong> (installed at <code>/usr/bin/mpirun</code>) instead of the <strong>container&rsquo;s MPI</strong> (at <code>/opt/hpcx/ompi/bin/mpirun</code> inside the rootfs).</p><h2 id=the-path-dance>The PATH Dance<a hidden class=anchor aria-hidden=true href=#the-path-dance>#</a></h2><p>TensorRT-LLM uses HPC-X OpenMPI from NVIDIA. The container has it at <code>/opt/hpcx/ompi/</code>. But when I chroot, the PATH still points to system binaries first.</p><p>Solution: Explicitly set PATH to prioritize container binaries.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=s2>&#34;/opt/hpcx/ompi/bin:</span><span class=nv>$PATH</span><span class=s2>&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>Run again&mldr; new error:</p><pre tabindex=0><code>mpirun: error while loading shared libraries: libucs.so.0: cannot open shared object file
</code></pre><p>The MPI binary is now correct, but it can&rsquo;t find its libraries!</p><h2 id=the-ld_library_path-saga>The LD_LIBRARY_PATH Saga<a hidden class=anchor aria-hidden=true href=#the-ld_library_path-saga>#</a></h2><p>The container&rsquo;s MPI needs libraries from <code>/opt/hpcx/ompi/lib/</code>, but <code>LD_LIBRARY_PATH</code> doesn&rsquo;t include it.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LD_LIBRARY_PATH</span><span class=o>=</span><span class=s2>&#34;/opt/hpcx/ompi/lib:</span><span class=nv>$LD_LIBRARY_PATH</span><span class=s2>&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>Run again&mldr; <strong>DIFFERENT</strong> error:</p><pre tabindex=0><code>symbol lookup error: /opt/hpcx/ompi/lib/libucc.so.1: undefined symbol: ucp_worker_progress
</code></pre><p>Wait, what? Now it&rsquo;s finding the HPC-X libraries but they&rsquo;re conflicting with system libraries!</p><h2 id=the-real-problem>The Real Problem<a hidden class=anchor aria-hidden=true href=#the-real-problem>#</a></h2><p>Here&rsquo;s what was happening:</p><ol><li>The <strong>system</strong> has OpenMPI installed (<code>libmpi.so</code>)</li><li>The <strong>container</strong> has HPC-X OpenMPI (<code>/opt/hpcx/ompi/lib/libmpi.so</code>)</li><li>TensorRT-LLM needs the HPC-X version</li><li>But the dynamic linker was mixing system and container libraries</li></ol><p>The fix: <strong>Put container libraries at the FRONT</strong> of <code>LD_LIBRARY_PATH</code>:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>LD_LIBRARY_PATH</span><span class=o>=</span><span class=s2>&#34;/opt/hpcx/ompi/lib:/usr/local/lib/python3.12/dist-packages/tensorrt_libs:</span><span class=nv>$LD_LIBRARY_PATH</span><span class=s2>&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>Finally, MPI works!</p><h2 id=the-cuda-version-surprise>The CUDA Version Surprise<a hidden class=anchor aria-hidden=true href=#the-cuda-version-surprise>#</a></h2><p>Got MPI working. Started a benchmark. New error:</p><pre tabindex=0><code>RuntimeError: Triton only supports CUDA 10.0 or higher, but got CUDA version: 13.0
</code></pre><p>Wait, CUDA 13.0? We&rsquo;re using CUDA 12.9!</p><p>Turns out: The system symlink <code>/usr/local/cuda</code> pointed to CUDA 13.0. The container needs CUDA 12.9.</p><p>Fix:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>CUDA_HOME</span><span class=o>=</span><span class=s2>&#34;/usr/local/cuda-12.9&#34;</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>PATH</span><span class=o>=</span><span class=s2>&#34;/usr/local/cuda-12.9/bin:</span><span class=nv>$PATH</span><span class=s2>&#34;</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=the-full-chroot-wrapper-script>The Full Chroot Wrapper Script<a hidden class=anchor aria-hidden=true href=#the-full-chroot-wrapper-script>#</a></h2><p>After all this pain, I created a proper chroot wrapper that handles everything:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span><span class=nv>SCRIPT_DIR</span><span class=o>=</span><span class=s2>&#34;/home/khan/container-rootfs&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Mount necessary filesystems</span>
</span></span><span class=line><span class=cl>sudo mount -t proc /proc <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/proc&#34;</span>
</span></span><span class=line><span class=cl>sudo mount --rbind /sys <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/sys&#34;</span>
</span></span><span class=line><span class=cl>sudo mount --rbind /dev <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/dev&#34;</span>
</span></span><span class=line><span class=cl>sudo mount --bind /home <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/home&#34;</span>
</span></span><span class=line><span class=cl>sudo mount --bind /data <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/data&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># DNS resolution</span>
</span></span><span class=line><span class=cl>sudo cp /etc/resolv.conf <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/etc/resolv.conf&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run in chroot with proper environment</span>
</span></span><span class=line><span class=cl>sudo chroot <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>&#34;</span> /usr/bin/env -i <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=nv>HOME</span><span class=o>=</span>/root <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=nv>PATH</span><span class=o>=</span><span class=s2>&#34;/opt/hpcx/ompi/bin:/usr/local/cuda-12.9/bin:/usr/local/bin:/usr/bin:/bin&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=nv>LD_LIBRARY_PATH</span><span class=o>=</span><span class=s2>&#34;/opt/hpcx/ompi/lib:/usr/local/lib/python3.12/dist-packages/tensorrt_libs:/usr/local/cuda-12.9/lib64&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=nv>CUDA_HOME</span><span class=o>=</span><span class=s2>&#34;/usr/local/cuda-12.9&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=nv>PYTHONPATH</span><span class=o>=</span><span class=s2>&#34;/usr/local/lib/python3.12/dist-packages&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    <span class=s2>&#34;</span><span class=nv>$@</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cleanup: unmount everything</span>
</span></span><span class=line><span class=cl>sudo umount <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/data&#34;</span>
</span></span><span class=line><span class=cl>sudo umount <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/home&#34;</span>
</span></span><span class=line><span class=cl>sudo umount <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/dev&#34;</span>
</span></span><span class=line><span class=cl>sudo umount <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/sys&#34;</span>
</span></span><span class=line><span class=cl>sudo umount <span class=s2>&#34;</span><span class=si>${</span><span class=nv>SCRIPT_DIR</span><span class=si>}</span><span class=s2>/proc&#34;</span>
</span></span></code></pre></td></tr></table></div></div><p>Now I can run:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>./run_in_rootfs.sh python3 /home/khan/benchmark-spark/benchmarks/trtllm_benchmark.py
</span></span></code></pre></td></tr></table></div></div><p>And it <strong>actually works</strong>.</p><h2 id=lessons-learned>Lessons Learned<a hidden class=anchor aria-hidden=true href=#lessons-learned>#</a></h2><ol><li><strong>Library paths matter</strong>: System vs container libraries will bite you</li><li><strong>Environment is everything</strong>: PATH, LD_LIBRARY_PATH, CUDA_HOME all critical</li><li><strong>MPI is picky</strong>: HPC-X OpenMPI isn&rsquo;t interchangeable with system OpenMPI</li><li><strong>Filesystem mounts</strong>: Need /proc, /sys, /dev, /home, and /data bind mounts</li><li><strong>DNS matters</strong>: Even forgot <code>/etc/resolv.conf</code> initially!</li></ol><h2 id=the-hard-part-is-over-right>The Hard Part Is Over&mldr; Right?<a hidden class=anchor aria-hidden=true href=#the-hard-part-is-over-right>#</a></h2><p>With a working chroot environment, I could finally start benchmarking. But getting here took <strong>hours</strong> of debugging library paths and runtime errors.</p><p>Sometimes I wonder if this is why people just accept Docker&rsquo;s overhead - at least it works out of the box!</p><p>But now that we have both Docker and native environments working, we can actually compare them fairly. And that&rsquo;s where things get interesting&mldr;</p><p>In the next post: What Grace Blackwell&rsquo;s unified memory architecture actually means, and why Docker&rsquo;s cgroups don&rsquo;t understand it.</p><hr><p><strong>Previous</strong>: <a href=../01-the-mystery>← Part 1: The Mystery</a><br><strong>Next</strong>: <a href=../03-unified-memory-revelation>Part 3: The Unified Memory Revelation →</a></p><p><strong>GitHub Repo</strong>: <a href=https://github.com/brandonrc/benchmark-spark>benchmark-spark</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://brandongeraci.com/tags/dgx-spark/>Dgx-Spark</a></li><li><a href=https://brandongeraci.com/tags/mpi/>Mpi</a></li><li><a href=https://brandongeraci.com/tags/chroot/>Chroot</a></li><li><a href=https://brandongeraci.com/tags/debugging/>Debugging</a></li></ul><nav class=paginav><a class=prev href=https://brandongeraci.com/projects/dgx-spark/03-unified-memory-revelation/><span class=title>« Prev</span><br><span>The Unified Memory Revelation: Why Docker Double-Counts</span>
</a><a class=next href=https://brandongeraci.com/projects/dgx-spark/01-the-mystery/><span class=title>Next »</span><br><span>The Mystery: Don't Just Blame the Hardware</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Down the Rabbit Hole: The MPI and Chroot Nightmare on x" href="https://x.com/intent/tweet/?text=Down%20the%20Rabbit%20Hole%3a%20The%20MPI%20and%20Chroot%20Nightmare&amp;url=https%3a%2f%2fbrandongeraci.com%2fprojects%2fdgx-spark%2f02-mpi-chroot-nightmare%2f&amp;hashtags=dgx-spark%2cmpi%2cchroot%2cdebugging"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Down the Rabbit Hole: The MPI and Chroot Nightmare on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fbrandongeraci.com%2fprojects%2fdgx-spark%2f02-mpi-chroot-nightmare%2f&amp;title=Down%20the%20Rabbit%20Hole%3a%20The%20MPI%20and%20Chroot%20Nightmare&amp;summary=Down%20the%20Rabbit%20Hole%3a%20The%20MPI%20and%20Chroot%20Nightmare&amp;source=https%3a%2f%2fbrandongeraci.com%2fprojects%2fdgx-spark%2f02-mpi-chroot-nightmare%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Down the Rabbit Hole: The MPI and Chroot Nightmare on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fbrandongeraci.com%2fprojects%2fdgx-spark%2f02-mpi-chroot-nightmare%2f&title=Down%20the%20Rabbit%20Hole%3a%20The%20MPI%20and%20Chroot%20Nightmare"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Down the Rabbit Hole: The MPI and Chroot Nightmare on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fbrandongeraci.com%2fprojects%2fdgx-spark%2f02-mpi-chroot-nightmare%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Down the Rabbit Hole: The MPI and Chroot Nightmare on whatsapp" href="https://api.whatsapp.com/send?text=Down%20the%20Rabbit%20Hole%3a%20The%20MPI%20and%20Chroot%20Nightmare%20-%20https%3a%2f%2fbrandongeraci.com%2fprojects%2fdgx-spark%2f02-mpi-chroot-nightmare%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Down the Rabbit Hole: The MPI and Chroot Nightmare on telegram" href="https://telegram.me/share/url?text=Down%20the%20Rabbit%20Hole%3a%20The%20MPI%20and%20Chroot%20Nightmare&amp;url=https%3a%2f%2fbrandongeraci.com%2fprojects%2fdgx-spark%2f02-mpi-chroot-nightmare%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Down the Rabbit Hole: The MPI and Chroot Nightmare on ycombinator" href="https://news.ycombinator.com/submitlink?t=Down%20the%20Rabbit%20Hole%3a%20The%20MPI%20and%20Chroot%20Nightmare&u=https%3a%2f%2fbrandongeraci.com%2fprojects%2fdgx-spark%2f02-mpi-chroot-nightmare%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://brandongeraci.com/>Brandon's Technical Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>